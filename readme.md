ğŸ“š LLM-Based Claim Verification with Long-Context Retrieval

Track A â€“ Kharagpur Data Science Hackathon 2026

An end-to-end LLM-assisted claim verification system that checks biographical claims against long literary sources using retrieval-augmented generation (RAG).
The system scales to book-length documents, retrieves relevant evidence, and produces verdicts with rationales.

ğŸš€ Overview

Modern language models struggle with long context and evidence attribution.
This project addresses that by combining:

Document cleaning and chunking

Semantic retrieval using sentence embeddings

LLM-based reasoning over retrieved evidence

Structured outputs with rationale and evidence snippets

The system verifies whether a claim is:

consistent

contradict

unknown (LLM unavailable or insufficient evidence)

ğŸ§  High-Level Architecture
Claim
  â”‚
  â–¼
Claim Extraction
  â”‚
  â–¼
Document Cleaning (Project Gutenberg)
  â”‚
  â–¼
Chunking (Sliding Window)
  â”‚
  â–¼
Embedding Index (Sentence Transformers)
  â”‚
  â–¼
Top-K Evidence Retrieval
  â”‚
  â–¼
LLM Reasoner (GPT-4o-mini)
  â”‚
  â–¼
Verdict + Rationale + Evidence
  â”‚
  â–¼
results/results.csv

ğŸ“‚ Project Structure
project/
â”‚
â”œâ”€â”€ app/                     # Pipeline orchestration
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ prompts.py
â”‚
â”œâ”€â”€ src/                     # Core logic
â”‚   â”œâ”€â”€ claims.py            # Claim extraction
â”‚   â”œâ”€â”€ clean_text.py        # Text normalization
â”‚   â”œâ”€â”€ chunking.py          # Long-context chunking
â”‚   â”œâ”€â”€ retrieval.py         # Semantic search
â”‚   â”œâ”€â”€ llm_reasoner.py      # LLM-based judgment
â”‚   â”œâ”€â”€ generate_results.py
â”‚   â””â”€â”€ reasoning.py
â”‚
â”œâ”€â”€ books/                   # Source corpus
â”‚   â”œâ”€â”€ in search of the castaways.txt
â”‚   â””â”€â”€ The Count of Monte Cristo.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ results.csv
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ report.pdf
â”‚
â”œâ”€â”€ evaluate_train_accuracy.py
â”œâ”€â”€ verify_llm.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ” Methodology
1ï¸âƒ£ Long-Context Handling

Entire books are cleaned and chunked

Each chunk is embedded using all-MiniLM-L6-v2

Avoids passing full books to the LLM

2ï¸âƒ£ Retrieval-Augmented Reasoning

Claims are matched to top-K relevant chunks

Only relevant evidence is sent to the LLM

Reduces hallucination and token usage

3ï¸âƒ£ LLM-Based Verdict Generation

Uses gpt-4o-mini for claim verification

Outputs both:

verdict (consistent / contradict / unknown)

rationale (short explanation)

4ï¸âƒ£ Robust Failure Handling

API rate-limit or network failures return:

verdict = "unknown"
rationale = "LLM unavailable"


Ensures pipeline never crashes

ğŸ“Š Evaluation

Temporary evaluation is performed using training labels:

accuracy â‰ˆ 0.64


This reflects:

Strong recall for consistent claims

Conservative behavior under uncertainty

Rate-limit effects on LLM availability

Accuracy improves significantly with uninterrupted LLM access.

âš ï¸ Known Limitations

LLM rate limits can cause unknown outputs

Semantic retrieval may miss subtle contradictions

No fine-tuning (zero-shot reasoning only)

Evidence quality depends on chunk boundaries

These limitations are explicitly handled and documented.

ğŸ” API Key Setup

Set your OpenAI API key as an environment variable:

Windows (PowerShell)
SETX OPENAI_API_KEY "your_api_key_here"


Restart the terminal after setting.

To verify:

python verify_llm.py

â–¶ï¸ How to Run
1ï¸âƒ£ Create virtual environment
python -m venv venv
venv\Scripts\activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run the pipeline
python -m app.main


Results will be generated in:

results/results.csv

ğŸ§ª Sample Output
id	prediction	rationale
80	contradict	Evidence conflicts with claim timeline
95	consistent	Claim aligns with retrieved text
78	unknown	LLM unavailable due to rate limit
ğŸ Conclusion

This project demonstrates a practical, scalable approach to long-context claim verification using modern LLMs.
It prioritizes robustness, explainability, and evidence grounding, making it suitable for real-world deployment and evaluation.

ğŸ‘¤ Authors:
Team Tech4change

Kharagpur Data Science Hackathon 2026
Track A Submission